\documentclass[conference]{IEEEtran}

\usepackage{amsmath, amssymb, amsfonts}
\usepackage{graphicx}
\graphicspath{{results/summary/}}
\usepackage{booktabs}
\usepackage{cite}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{Approximation Algorithms for the $k$-Center Problem:\\
An Empirical Analysis of Metric Geometry Impacts}

% Update with your actual details
\author{\IEEEauthorblockN{Author Name}
\IEEEauthorblockA{Department of Computer Science\\
Universidade Federal de Minas Gerais (UFMG)\\
Belo Horizonte, Brazil\\
Email: email@dcc.ufmg.br}}

\begin{document}

\maketitle

\begin{abstract}
This work presents an empirical evaluation of two 2-approximation algorithms for the metric $k$-center problem—Farthest-First Traversal and Interval Refinement—benchmarked against a K-Means baseline. We investigate the interaction between cluster geometry and distance metrics, specifically Minkowski ($p=1, 2$) and Mahalanobis distances. Experiments were conducted on 46 datasets, comprising both real-world UCI data and synthetic sets designed to test anisotropic and overlapping cluster regimes. Results indicate that while K-Means achieves superior average Silhouette scores (0.566), the interval-refinement $k$-center algorithm offers a competitive alternative (Silhouette 0.547) with significantly lower runtime variance. Furthermore, we demonstrate that the $k$-center objective yields distinct behaviors under Mahalanobis distance, challenging the assumption that covariance-based metrics strictly dominate Euclidean metrics in elliptical cluster recovery.
\end{abstract}

\section{Introduction}

The $k$-center problem is a fundamental formulation in combinatorial optimization, clustering, and facility location. Given a set of points $P$ and an integer $k$, the objective is to select a set $S \subseteq P$ with $|S| = k$ to minimize the maximum distance from any point in $P$ to its nearest neighbor in $S$. Unlike the $k$-means objective, which minimizes within-cluster variance, $k$-center minimizes the worst-case covering radius.

Since the general $k$-center problem is NP-hard, approximation algorithms are essential for practical applications. This study implements and analyzes two 2-approximation algorithms: the greedy Farthest-First Traversal (Gonzalez's algorithm) and a parametric Interval-Refinement approach. We evaluate how the theoretical approximation guarantees translate into practical performance and analyze the impact of metric selection—specifically Minkowski and Mahalanobis distances—on the clustering quality of datasets with diverse geometries.

\section{Methods}

\subsection{Distance Metrics}
We consider the Minkowski distance of order $p \ge 1$ between vectors $x, y \in \mathbb{R}^d$:
\begin{equation}
 d_p(x, y) = \|x - y\|_p = \left( \sum_{i=1}^d |x_i - y_i|^p \right)^{1/p}.
\end{equation}
This study focuses on Manhattan ($p=1$) and Euclidean ($p=2$) norms. Additionally, to address anisotropic data distributions, we employ the Mahalanobis distance:
\begin{equation}
 d_M(x,y) = \sqrt{(x-y)^\top \Sigma^{-1} (x-y)},
\end{equation}
where $\Sigma$ represents the empirical covariance matrix of the dataset. This metric effectively standardizes the data along principal components, accounting for feature correlations.

\subsection{2-Approximate $k$-Center Algorithms}
We examine two constructive heuristics that satisfy the approximation factor $\alpha \le 2$.

\subsubsection{Farthest-First Traversal}
This greedy strategy initializes the first center arbitrarily (or randomly) and iteratively selects the point $p \in P$ that maximizes the distance to the current set of centers $S$. The time complexity is $O(nk)$, where $n=|P|$. It provides a computationally inexpensive 2-approximation but is sensitive to outliers.

\subsubsection{Interval Refinement}
This algorithm couples a greedy coverage decision procedure with a search over the feasible radius space. Maintaining lower ($L$) and upper ($U$) bounds on the optimal radius $r^*$, the algorithm tests a candidate radius $r$. If the greedy procedure can cover $P$ with $k$ balls of radius $r$, $U$ is updated; otherwise, $L$ is updated. The complexity is $O(nk \cdot \log((U-L)/\epsilon))$, where $\epsilon$ is the convergence tolerance.

\subsection{K-Means Baseline}
To provide a context for solution quality, we utilize the K-Means++ algorithm (via scikit-learn) as a baseline. While K-Means optimizes a different objective (inertia), it serves as a standard benchmark for cluster coherence (Silhouette) and ground-truth recovery (Adjusted Rand Index - ARI).

\section{Implementation and Experimental Design}

\subsection{Architecture}
The implementation is structured as a modular Python package. Distance computations are vectorized using NumPy to ensure efficiency. For the Mahalanobis metric, the covariance matrix is regularized to ensure invertibility. The core algorithms operate on precomputed distance matrices managed by a caching strategy to minimize redundant $O(n^2)$ computations across multiple runs.

\subsection{Dataset Selection and Constraints}
The experimental corpus consists of numeric datasets from the UCI Machine Learning Repository and synthetic datasets generated via Gaussian mixtures. The synthetic sets were explicitly designed to vary cluster eccentricity and overlap, enabling a stress test of the distance metrics.

To facilitate extensive hyperparameter sweeping (15 independent runs per configuration across 856 total settings) within computational time constraints, we applied a strict filtering criterion. Datasets exceeding $N=10,000$ samples were excluded. This decision balanced the need for statistical significance across diverse dataset types with the quadratic space complexity $O(n^2)$ required for the precomputed distance matrices used by the exact implementations of the approximation algorithms.

The final evaluation set comprises 46 datasets (6 UCI, 40 synthetic).

\subsection{Evaluation Metrics}
Solution quality is assessed using:
\begin{itemize}
    \item \textbf{Radius ($r$):} The objective function value (lower is better).
    \item \textbf{Silhouette Score:} Measures cluster cohesion and separation.
    \item \textbf{Adjusted Rand Index (ARI):} Measures agreement with ground-truth labels.
    \item \textbf{Runtime:} Wall-clock execution time.
\end{itemize}

\section{Results and Discussion}

\subsection{Algorithm Performance}
Table \ref{tab:algorithm_comparison} summarizes the aggregate performance. As expected, K-Means outperforms the approximation algorithms in Silhouette score (0.566), benefiting from its centroid-optimization nature which aligns closely with the Silhouette metric.

However, the $k$-center algorithms demonstrate extreme computational efficiency. Farthest-First traversal is effectively instantaneous for the tested scales ($< 1$ms on average), while Interval Refinement requires approximately 6ms. This represents a speedup factor of roughly $9\times$ compared to K-Means (54ms), highlighting the utility of $k$-center approximations for rapid initialization or real-time constraints.

\begin{table}[!t]
  \centering
  \caption{Aggregate Performance by Algorithm (Mean $\pm$ SD)}
  \label{tab:algorithm_comparison}
  \resizebox{\linewidth}{!}{%
    \begin{tabular}{@{}lrrrr@{}}
    \toprule
     Algorithm & Radius & Silhouette & ARI & Runtime (s) \\
    \midrule
    Farthest-First & 720.17 $\pm$ 5004.5 & 0.491 $\pm$ 0.168 & 0.462 $\pm$ 0.311 & 0.000 \\
    Interval Refinement & 854.41 $\pm$ 5542.6 & 0.547 $\pm$ 0.181 & 0.503 $\pm$ 0.343 & 0.006 \\
    K-Means & \textemdash{} & 0.566 $\pm$ 0.149 & 0.625 $\pm$ 0.356 & 0.054 \\
    \bottomrule
    \end{tabular}
  }
\end{table}

\subsection{Impact of Distance Metrics}
A key finding of this study concerns the behavior of Mahalanobis distance on anisotropic clusters. Theoretically, Mahalanobis distance should favor elliptical clusters by normalizing variance. However, our empirical results (Table \ref{tab:metric_comparison}) show that while Mahalanobis distance slightly improves Silhouette scores compared to Farthest-First using Euclidean distance, it does not consistently yield higher ARI.

This suggests a potential misalignment between the $k$-center objective (min-max radius) and the global covariance normalization of the Mahalanobis metric. The $k$-center problem is highly sensitive to outliers; Mahalanobis rescaling can sometimes amplify the relative distance of outliers in low-variance directions, thereby degrading the min-max radius objective.

\begin{table}[!t]
  \centering
  \caption{Aggregate Performance by Distance Metric}
  \label{tab:metric_comparison}
  \resizebox{\linewidth}{!}{%
    \begin{tabular}{@{}lrrrr@{}}
    \toprule
     Metric & Radius & Silhouette & ARI & Runtime (s) \\
    \midrule
    Euclidean & \textemdash{} & 0.566 $\pm$ 0.149 & 0.625 $\pm$ 0.356 & 0.054 \\
    Mahalanobis & 638.65 $\pm$ 4059.6 & 0.512 $\pm$ 0.183 & 0.447 $\pm$ 0.348 & 0.005 \\
    Minkowski & 926.92 $\pm$ 6024.4 & 0.549 $\pm$ 0.178 & 0.520 $\pm$ 0.330 & 0.005 \\
    \bottomrule
    \end{tabular}
  }
\end{table}

\subsection{Refinement Granularity}
The Interval Refinement algorithm exposes a trade-off between precision and runtime. We observed diminishing returns at finer granularities. Reducing the interval width from 12\% to 4\% yielded negligible improvements in Silhouette score (0.541 vs 0.535) but increased runtime by roughly 14\%. A width of 12-18\% appears to be the Pareto-optimal configuration for this implementation.

\begin{figure}[!t]
  \centering
  \includegraphics[width=\linewidth]{silhouette_by_algorithm.png}
  \caption{Average Silhouette score comparison. K-Means maintains a lead, but Interval Refinement closes the gap significantly compared to the greedy Farthest-First approach.}
\end{figure}

\section{Conclusion}

This study confirms that 2-approximation algorithms for $k$-center provide a highly efficient alternative to K-Means, offering order-of-magnitude speedups at the cost of moderate reductions in cluster cohesion (Silhouette) and ground-truth recovery (ARI). The Interval Refinement variant offers a valuable middle ground, improving solution quality over the greedy approach with minimal computational overhead. Future work should investigate identifying a localized Mahalanobis metric (per-cluster covariance) to better address the limitations observed with global covariance normalization in complex geometric arrangements.

\bibliographystyle{IEEEtran}

\end{document}