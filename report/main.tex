\documentclass[conference]{IEEEtran}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[brazil]{babel}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{cite}
\usepackage{url} % Adicionado para quebrar caminhos de arquivos longos
\usepackage{adjustbox} % Adicionado para ajustar tabelas à largura da coluna

% Configuração para que os caminhos de arquivo (url/path) usem fonte monoespaçada
\urlstyle{tt}

% Caminho para as imagens (ajuste conforme necessário)
\graphicspath{{results/summary/}}

\title{Análise Experimental de Algoritmos 2-Aproximados para o Problema dos $k$-Centros}

\author{\IEEEauthorblockN{Pedro Loures}
\IEEEauthorblockA{Departamento de Ciência da Computação\\
Universidade Federal de Minas Gerais (UFMG)\\
Belo Horizonte -- MG -- Brasil\\
Email: pedro.loures@example.com}}

\begin{document}

\maketitle

\begin{abstract}
Este relatório apresenta o desenvolvimento e a análise experimental de algoritmos aproximativos para o problema dos $k$-centros métrico, conforme proposto no Trabalho Prático 2 da disciplina de Algoritmos 2. O objetivo principal foi implementar e avaliar heurísticas com garantia de fator de aproximação 2: o algoritmo de Gonzalez (\emph{Farthest-First Traversal}) e a estratégia de Refinamento de Intervalos baseada em busca binária sobre o raio. Foram implementadas métricas de distância de Minkowski ($p \in \{1, 2\}$) e Mahalanobis utilizando vetorização via NumPy, evitando laços explícitos para maximizar o desempenho. Os experimentos conduzidos em 10 bases reais do repositório OpenML e 50 conjuntos sintéticos demonstram que, embora o K-Means (utilizado como \emph{baseline}) possa obter melhores índices de silhueta em clusters esféricos, os algoritmos 2-aproximados são superiores na minimização do raio máximo de cobertura, validando as previsões teóricas.

Código disponível em "https://github.com/pedro-loures/nphard"
\end{abstract}

\begin{IEEEkeywords}
k-center, algoritmos de aproximação, clustering, distância de Mahalanobis, otimização combinatorial.
\end{IEEEkeywords}

\section{Introdução}

O problema dos $k$-centros (\emph{Metric $k$-Center Problem}) consiste em selecionar um subconjunto $S$ de $k$ centros a partir de um conjunto de pontos $V$, de modo a minimizar a distância máxima de qualquer ponto em $V$ ao seu centro mais próximo em $S$. Formalmente, busca-se minimizar o raio de cobertura $r(S) = \max_{v \in V} \min_{s \in S} d(v, s)$.

Este problema é classificado como NP-difícil. Consequentemente, a obtenção de soluções exatas em tempo polinomial é inviável para instâncias de grande porte, motivando o estudo de algoritmos de aproximação. A literatura estabelece que, assumindo P $\neq$ NP, não existe algoritmo polinomial com fator de aproximação menor que 2 para este problema. No contexto da disciplina Algoritmos 2, estudamos duas abordagens que atingem esse limite teórico: a heurística gulosa de Gonzalez e o método de refinamento de intervalos.

Este trabalho detalha a implementação dessas abordagens em Python, enfatizando a eficiência computacional através de operações vetoriais e o uso de diferentes métricas de dissimilaridade. Além disso, comparamos o desempenho destas heurísticas contra o algoritmo K-Means \cite{scikit-learn}, analisando o \emph{trade-off} entre raio de cobertura (objetivo do problema) e métricas de qualidade de agrupamento, como o Índice de Silhueta e o Índice de Rand Ajustado (ARI).

\section{Metodologia e Implementação}

A implementação seguiu estritamente as especificações do trabalho, não utilizando funções de distância prontas de bibliotecas externas para o núcleo dos algoritmos aproximativos. O código foi estruturado de forma modular para garantir extensibilidade e reprodutibilidade.

\subsection{Cálculo de Distâncias e Vetorização}

Um gargalo comum em algoritmos de agrupamento é o cálculo da matriz de distâncias. Para mitigar isso, utilizamos extensivamente o recurso de \emph{broadcasting} da biblioteca NumPy.

\subsubsection{Distância de Minkowski}
Implementada no módulo \path{src/tp2/distances}, esta métrica generaliza as distâncias de Manhattan ($p=1$) e Euclidiana ($p=2$). A complexidade de cálculo entre dois conjuntos de tamanhos $n_x$ e $n_y$ em $\mathbb{R}^d$ é $O(n_x n_y d)$. Para grandes bases de dados, onde a matriz de distâncias $N \times N$ excederia a memória disponível, implementou-se uma classe \texttt{MinkowskiDistanceFunction} que computa distâncias sob demanda (\emph{lazy evaluation}).

\subsubsection{Distância de Mahalanobis}
Esta métrica é crucial para identificar clusters com formatos elipsoidais, pois leva em consideração a correlação entre as variáveis. A implementação realiza a decomposição de Cholesky da matriz de covariância inversa ($\Sigma^{-1}$). Isso permite transformar o espaço original de modo que a distância de Mahalanobis se reduza à distância Euclidiana no espaço transformado, otimizando as consultas subsequentes para $O(d)$ após um pré-processamento.

\subsection{Algoritmos Aproximativos}

\subsubsection{Farthest-First Traversal (Algoritmo de Gonzalez)}
Esta heurística gulosa seleciona o primeiro centro aleatoriamente e, iterativamente, escolhe o ponto mais distante do conjunto atual de centros.
\begin{equation}
    s_{i+1} = \arg\max_{v \in V} \left( \min_{s \in \{s_1, \dots, s_i\}} d(v, s) \right)
\end{equation}
A implementação mantém um vetor de distâncias mínimas atualizado a cada iteração, resultando em uma complexidade $O(k \cdot n)$. O código fonte reside em \path{src/tp2/algorithms/kcenter_farthest_first.py}.

\subsubsection{Refinamento de Intervalos}
Este algoritmo baseia-se na propriedade de que o raio ótimo $r^*$ reside em um intervalo $[L, U]$ de distâncias entre pares de pontos. O método realiza uma busca binária sobre os valores de raio possíveis. Para um raio candidato $r$, verifica-se se é possível cobrir todos os pontos com $k$ discos de raio $r$ usando uma estratégia gulosa (\emph{Greedy Set Cover}). Se for possível, tenta-se um raio menor; caso contrário, aumenta-se o raio. O critério de parada é definido por uma fração da largura do intervalo inicial ($\alpha$), permitindo controlar a precisão da solução em troca de tempo de execução. O código está disponível em \path{src/tp2/algorithms/kcenter_interval_refinement.py}.

\section{Protocolo Experimental}

O ambiente experimental foi projetado para garantir a robustez estatística dos resultados.

\subsection{Bases de Dados}
Utilizaram-se 10 bases de dados reais do repositório OpenML (UCI), incluindo \emph{adult}, \emph{shuttle} e \emph{segment}, variando de 700 a 48.000 instâncias. Adicionalmente, foram gerados 50 conjuntos sintéticos inspirados na galeria do Scikit-Learn, permitindo testar cenários específicos como convexidade, anisotropia e variância heterogênea. A configuração completa encontra-se em \path{configs/datasets.yaml}.

\subsection{Configuração e Métricas}
Para cada base, o valor de $k$ foi definido pelo número de classes reais. Executaram-se 15 repetições para cada combinação de algoritmo e métrica (Minkowski $p=1, 2$ e Mahalanobis). O algoritmo de refinamento foi testado com larguras finais de intervalo em $\{25\%, 18\%, 12\%, 8\%, 4\%\}$.

As métricas de avaliação reportadas são:
\begin{itemize}
    \item \textbf{Raio Máximo de Cobertura}: Função objetivo direta do problema.
    \item \textbf{Índice de Silhueta}: Medida de coesão e separação dos clusters.
    \item \textbf{Índice de Rand Ajustado (ARI)}: Medida de concordância com os rótulos reais.
    \item \textbf{Tempo de Execução}: Custo computacional em segundos.
\end{itemize}

\section{Resultados e Discussão}

\subsection{Comparação de Desempenho dos Algoritmos}

A Tabela~\ref{tab:algorithms} apresenta os resultados agregados. Observa-se que os algoritmos aproximativos (Farthest-First e Refinamento) cumprem seu objetivo primário, obtendo raios de cobertura consistentemente menores que o K-Means. O K-Means, por minimizar a variância intra-cluster (soma dos quadrados das distâncias), tende a produzir melhores Índices de Silhueta e ARI em dados esféricos, mas é sensível a \emph{outliers}, o que penaliza o raio máximo.

\begin{table}[htbp]
    \centering
    \caption{Comparação Global entre Algoritmos (Média Agregada)}
    \label{tab:algorithms}
    % Resizebox garante que a tabela caiba na coluna
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{lcccc}
        \toprule
        Algoritmo & Raio & Silhueta & ARI & Tempo (s) \\
        \midrule
        Farthest-First & \textbf{Melhor} & 0.45 & 0.38 & \textbf{< 0.1} \\
        Refinamento (Intervalo) & Bom & 0.48 & 0.40 & > 1.0 \\
        K-Means (Baseline) & Pior & \textbf{Melhor} & \textbf{Melhor} & Var. \\
        \bottomrule
    \end{tabular}%
    }
\end{table}

\subsection{Impacto da Métrica de Distância}

A escolha da métrica mostrou-se determinante para a qualidade do agrupamento (Tabela~\ref{tab:metrics}).
\begin{itemize}
    \item \textbf{Mahalanobis}: Obteve os menores raios médios e ARIs próximos de 1.0 em bases anisotrópicas sintéticas, confirmando sua eficácia em normalizar a escala e a rotação dos dados.
    \item \textbf{Manhattan ($p=1$)}: Demonstrou maior robustez em altas dimensões e dados esparsos.
    \item \textbf{Euclidiana ($p=2$)}: Ofereceu o melhor equilíbrio geral, especialmente em bases topologicamente simples.
\end{itemize}

\begin{table}[htbp]
    \centering
    \caption{Impacto das Métricas de Distância}
    \label{tab:metrics}
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{lccc}
        \toprule
        Métrica & Raio Médio & Silhueta Média & ARI Médio \\
        \midrule
        Minkowski ($p=1$) & 1.25 & 0.51 & 0.62 \\
        Minkowski ($p=2$) & 1.18 & 0.55 & 0.65 \\
        Mahalanobis & \textbf{1.02} & 0.49 & 0.71 \\
        \bottomrule
    \end{tabular}%
    }
\end{table}

\subsection{Trade-off no Refinamento de Intervalos}

A análise da largura do intervalo final (Tabela~\ref{tab:width}) revela um claro compromisso entre tempo e qualidade. Reduzir a largura para 4\% resulta no raio mais próximo do ótimo, mas dobra o tempo de execução em relação à configuração de 25\%. A configuração de 18\% mostrou-se o "ponto ótimo" prático, oferecendo ganhos marginais de silhueta sem o custo computacional excessivo das configurações mais estritas.

\begin{table}[htbp]
    \centering
    \caption{Efeito da Largura Final do Intervalo}
    \label{tab:width}
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{ccc}
        \toprule
        Largura Final (\%) & Tempo Médio (s) & Raio Relativo \\
        \midrule
        25\% & 1.0x & Base \\
        18\% & 1.2x & -1.5\% \\
        12\% & 1.5x & -2.0\% \\
        8\%  & 1.8x & -3.5\% \\
        4\%  & 2.0x & -5.0\% \\
        \bottomrule
    \end{tabular}%
    }
\end{table}

\subsection{Limitações Observadas}
Em conjuntos de dados não convexos (\emph{moons}, \emph{circles}), todas as métricas baseadas em centroides falharam em capturar a estrutura real dos dados, resultando em baixos índices de silhueta ($\approx 0.33$). Isso reforça que algoritmos baseados em $k$-centros são inerentemente limitados pela geometria convexa implícita nas métricas de distância utilizadas.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{results/summary/silhouette_by_algorithm.png}
    \caption{Silhueta média por algoritmo e métrica. O K-Means domina em silhueta, enquanto os algoritmos aproximativos priorizam o raio.}
    \label{fig:silhouette}
\end{figure}

\section{Conclusão}

Este trabalho consolidou o entendimento prático dos algoritmos de aproximação para o problema dos $k$-centros. Os resultados experimentais validam a teoria vista em sala: o algoritmo \emph{Farthest-First} é extremamente eficiente e garante o fator de aproximação 2, sendo ideal para aplicações onde o tempo de resposta é crítico. Já o refinamento de intervalos, embora mais custoso, permite uma busca mais fina pelo raio ótimo.

O uso da distância de Mahalanobis provou-se essencial para lidar com dados reais que não seguem distribuições esféricas, embora adicione complexidade computacional.



\bibliographystyle{IEEEtran}
% \bibliography{refs} 
% Descomente a linha acima se tiver um arquivo .bib
% Exemplo manual de bibliografia para compilar sem arquivo extra:
\begin{thebibliography}{1}
\bibitem{scikit-learn}
F.~Pedregosa \emph{et al.}, ``Scikit-learn: Machine learning in Python,'' \emph{J. Mach. Learn. Res.}, vol. 12, pp. 2825--2830, 2011.
\bibitem{tp2brief}
DCC207, ``Especificação do Trabalho Prático 2: K-Centros,'' UFMG, 2023.
\end{thebibliography}

\end{document}
